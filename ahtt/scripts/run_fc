# running the toy-based FC scan
# the recipe is based on the latest results (02/08/2022)
# the recipe is fully sequential, whenever jobs are submitted wait until it finishes!!

#0 some preliminary stuff - needed every session
#0a this gets the condor_check command which helps telling which jobs dont complete
source /data/dust/user/afiqaize/cms/sft/condor/condorUtil.sh # at desy naf
source /afs/cern.ch/work/a/afiqaize/public/randomThings/misc/condor/condorUtil.sh # at cern lxplus, NOT SUPPORTED ANYMORE

#1 list down the pairs to run on, and a few other variables
#1a default settings here are what has been found to have good job efficiency (DESY default HTC slot) for the combined fits
#1b i.e. 50 toys/job and 20 jobs per point
#1c the pair specification below here uses the grid syntax i.e. the first 'm...;w...' refers to A masses and widths, and the second H masses and widths
#1d it is also possible to directly specify the desired pairs like 'A_m1_w1,H_m2_w2;A_m3_w3,H_m4_w4;...'
masses='m365,m400,m650,m1000'
widths='w2p5,w5p0,w10p0'
pairs="${masses};${widths};${masses};${widths}"
#pairs='m365;w2p0;m365;w2p0'
tag='lx'
channels='ee,em,mm,e3j,e4pj,m3j,m4pj'
years='2016pre,2016post,2017,2018'
ntoy='43'
idxs='..13'
#exps='exp-b,exp-s,exp-01,exp-10'
exps='exp-b'
keeps='eff_b,eff_e,eff_m_id,eff_m_iso,eff_trigger,fake,JEC,JER,MET,QCDscale,hdamp,tmass,EWK_scheme,alphaS,PDF_PCA_0,L1,EWQCD,pileup,lumi,norm,UEtune,CR_ERD,CR_QCD,herwig,bb4l'
drops='Type3,FlavorQCD_201,eff_b_13TeV_201,TT_norm,EWK_yukawa,bindingEnergy'

#2 make the datacard
./../scripts/submit_twin.py --mode 'datacard,validate' --point "${pairs}" --sushi-kfactor --lnN-under-threshold --year "${years}" --channel "${channels}" --tag "${tag}" --keep "${keeps}" --drop "${drops}" --unblind

#3 generating the shared (0, 0) toys - IGNORE FOR NOW
#3a toy locations:
#naf-desy: /data/dust/group/cms/exotica-desy/HeavyHiggs/toys_ULFR2/unblind_stage3_20231129/lx/ # or ll
#lxplus: 
./../scripts/submit_twin.py --mode generate --point "${pairs}" --tag "${tag}" --g-values '0.0,0.0' --n-toy "${ntoy}" --run-idxs "${idxs}" --toy-location <directory of shared toys> --unblind

#4 run the (0, 0) point, which can use shared toys - IGNORE FOR NOW
#4a only relevant if shared toys have been made / are needed - otherwise skip to #6
#4b this submits N jobs based on ${idxs}, each running ${ntoy} toys
#4c indexing can either be individually enumerated 'i1,i2,...,iN' or deduced 'init..final' (two dots!!); with init defaulting to 0 if omitted
./../scripts/submit_twin.py --mode contour --point "${pairs}" --tag "${tag}" --g-values '0.0,0.0' --fc-expect "${exps}" --n-toy "${ntoy}" --run-idxs "${idxs}" --fc-single-point --toy-location <directory of shared toys> --unblind #--save-toy

#5 check if the jobs ran fine and delete the faulty logs
#5a if this command outputs anything on the screen, repeat the previous step you were running immediately
#5b alternatively always repeat the previous step regardless, and proceed only when it submits no jobs
#5c this step is useful after every step that submits HTC jobs
condor_check . | tee /dev/tty | xargs rm

#6 run the remaining LO points
#6a use --fc-initial-distance to start with a different resolution than default of 1.0 (runs over more points initially, but less refine steps)
./../scripts/submit_twin.py --mode contour --point "${pairs}" --tag "${tag}" --fc-expect "${exps}" --n-toy "${ntoy}" --run-idxs "${idxs}" --unblind --no-log
#6b repeat step 5

#7 merge the toy files
./../scripts/submit_twin.py --mode 'clean,merge' --point "${pairs}" --tag "${tag}"

#8 compile the available statistics - the non-shared toys are deleted, but all the relevant numbers we have gotten already
./../scripts/submit_twin.py --mode 'clean,compile' --point "${pairs}" --tag "${tag}" --fc-expect "${exps}" --delete-root --local --force --unblind

#9 when extra toys are needed
#9a use this to add a fixed amount of toys = nidxs * ntoy
./../scripts/submit_twin.py --mode contour --point "${pairs}" --tag "${tag}" --fc-expect "${exps}" --n-toy "${ntoy}" --run-idxs "${idxs}" --fc-mode add --fc-skip-data --unblind --no-log
#9b use this to add a variable number of toys per point, such that the sum for each point is about the same
./../scripts/submit_twin.py --mode contour --point "${pairs}" --tag "${tag}" --fc-expect "${exps}" --n-toy "${ntoy}" --run-idxs "${idxs}" --fc-mode brim --fc-skip-data --unblind --no-log
#9c repeat step #5 and go back to #7/#8, whichever of --fc-mode add/brim is used

#10 if enough toys are made for each point, refine the grid
#10a always do #7 and #8 every time after #9 and #10
./../scripts/submit_twin.py --mode contour --point "${pairs}" --tag "${tag}" --fc-expect "${exps}" --n-toy "${ntoy}" --run-idxs "${idxs}" --fc-mode refine --unblind --no-log
#10b repeat step #5 and go back to #7
#10c rule of thumb: use --no-log when more than ~2k jobs per A/H pair are submitted, or always if the jobs usually succeed, and the logs aren't needed
